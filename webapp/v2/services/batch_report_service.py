"""
æ‰¹é‡æŠ¥è¡¨ç”ŸæˆæœåŠ¡ã€‚
"""
import os
import zipfile
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import logging
from sqlalchemy.orm import Session

from ..crud import batch_reports as crud_batch_reports
from ..models.reports import BatchReportTask, BatchReportTaskItem
from ..pydantic_models.reports import BatchReportTaskItemUpdate, ReportFileManagerCreate
from .report_generators import (
    PayrollSummaryGenerator,
    PayrollDetailGenerator,
    DepartmentSummaryGenerator,
    TaxDeclarationGenerator,
    SocialInsuranceGenerator,
    AttendanceSummaryGenerator
)

# è®¾ç½®logger
logger = logging.getLogger(__name__)


class BatchReportService:
    """æ‰¹é‡æŠ¥è¡¨ç”ŸæˆæœåŠ¡"""
    
    def __init__(self, db: Session):
        self.db = db
        self.output_base_dir = "reports/batch_exports"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_base_dir, exist_ok=True)
        
        # æŠ¥è¡¨ç±»å‹æ˜ å°„ - å°†æ•°æ®åº“ä¸­çš„æŠ¥è¡¨ç±»å‹æ˜ å°„åˆ°ç°æœ‰çš„ç”Ÿæˆå™¨
        self.report_type_mapping = {
            # è–ªèµ„ç›¸å…³æŠ¥è¡¨æ˜ å°„åˆ°è–ªèµ„æ˜ç»†ç”Ÿæˆå™¨
            "yingfa_PY": "payroll_detail",      # 04å®å‘è¡¨-æ­£ç¼–
            "yingfa_ZB": "payroll_detail",      # 01ã€02ã€03åº”å‘è¡¨-æ­£ç¼–  
            "yingfa-PY": "payroll_detail",      # 06åº”å‘è¡¨-åŒºè˜
            "shifa-PY": "payroll_detail",       # 10è˜ç”¨å®å‘è¡¨æœˆåº¦
            
            # éƒ¨é—¨æ±‡æ€»ç›¸å…³æŠ¥è¡¨æ˜ å°„åˆ°éƒ¨é—¨æ±‡æ€»ç”Ÿæˆå™¨
            "ZBDRDPT": "department_summary",    # 05æ­£ç¼–å¯¼å…¥å¤§å¹³å°å·¥èµ„æ•°æ®
            "PYGZDRDPT": "department_summary",  # 13è˜ç”¨å·¥èµ„å¯¼å…¥å¤§å¹³å°è¡¨æœˆåº¦
            "PYGZHZYD": "department_summary",   # 12è˜ç”¨å·¥èµ„æ±‡æ€»æœˆåº¦
            
            # ä¸“é¡¹æŠ¥è¡¨æ˜ å°„åˆ°è–ªèµ„æ±‡æ€»ç”Ÿæˆå™¨
            "ZJYFYD": "payroll_summary",        # 09ä¸“æŠ€åº”å‘æœˆåº¦
            "YTFYFYD": "payroll_summary",       # 07åŸæŠ•æœåº”å‘æœˆåº¦
            "ZXYFYD": "payroll_summary",        # 08ä¸“é¡¹åº”å‘æœˆåº¦
            "ZJSFBYD": "payroll_summary",       # 11ä¸“æŠ€å®å‘è¡¨æœˆåº¦
            
            # ğŸš€ æ–°å¢ï¼šå¤±è´¥çš„æŠ¥è¡¨ç±»å‹æ˜ å°„
            "gong_wu_yuan_can_gong_shi_ye_shi_fa": "payroll_detail",  # å…¬åŠ¡å‘˜å‚å·¥äº‹ä¸šå®å‘
            "zheng_bian_dao_ru_da_ping_tai_gong_zi_shu_ju": "department_summary",  # æ­£ç¼–å¯¼å…¥å¤§å¹³å°å·¥èµ„æ•°æ®
            "pin_yong_shi_fa_biao_yue_du_shi_tu": "payroll_detail",  # è˜ç”¨å®å‘è¡¨æœˆåº¦è§†å›¾
            "zhuan_ji_shi_fa_biao_yue_du_shi_tu": "payroll_detail",  # ä¸“æŠ€å®å‘è¡¨æœˆåº¦è§†å›¾
            "pin_yong_gong_zi_hui_zong_yue_du_shi_tu": "payroll_summary",  # è˜ç”¨å·¥èµ„æ±‡æ€»æœˆåº¦è§†å›¾
            "pin_yong_gong_zi_dao_ru_da_ping_tai_biao_yue_du": "department_summary",  # è˜ç”¨å·¥èµ„å¯¼å…¥å¤§å¹³å°è¡¨æœˆåº¦
            
            # ç¤¾ä¿ç›¸å…³æŠ¥è¡¨æ˜ å°„åˆ°ç¤¾ä¿ç”Ÿæˆå™¨
            "NJZZMX": "social_insurance",       # 43å¹´é‡‘åšè´¦æ˜ç»†
            "GJJZZMXB": "social_insurance",     # 39å…¬ç§¯é‡‘åšè´¦æ˜ç»†è¡¨
            "JBZZMX": "social_insurance",       # 42æœºä¿åšè´¦æ˜ç»†
            "SBSYGSZZMX": "social_insurance",   # 44ç¤¾ä¿ã€å¤±ä¸šå·¥ä¼¤åšè´¦æ˜ç»†
            "YLBXZZMX": "social_insurance",     # 45åŒ»ç–—ä¿é™©åšè´¦æ˜ç»†
            
            # ç”¨æ¬¾è®¡åˆ’æŠ¥è¡¨æ˜ å°„åˆ°è–ªèµ„æ±‡æ€»ç”Ÿæˆå™¨
            "ZBGZYKJH": "payroll_summary",      # 52æ­£ç¼–å·¥èµ„ç”¨æ¬¾è®¡åˆ’
            "PYGZYKJH": "payroll_summary",      # 53è˜ç”¨å·¥èµ„ç”¨æ¬¾è®¡åˆ’
            "SBGJJYKJH": "social_insurance",    # 54ç¤¾ä¿å…¬ç§¯é‡‘ç”¨æ¬¾è®¡åˆ’
        }
    
    async def execute_batch_report_task(self, task_id: int) -> None:
        """
        æ‰§è¡Œæ‰¹é‡æŠ¥è¡¨ç”Ÿæˆä»»åŠ¡
        
        Args:
            task_id: ä»»åŠ¡ID
        """
        try:
            # è·å–ä»»åŠ¡
            task = crud_batch_reports.get_batch_report_task(self.db, task_id)
            if not task:
                logger.error(f"æ‰¹é‡æŠ¥è¡¨ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
                return
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
            crud_batch_reports.update_batch_report_task(
                self.db,
                task_id,
                crud_batch_reports.BatchReportTaskUpdate(
                    status="running",
                    started_at=datetime.utcnow()
                )
            )
            
            logger.info(f"å¼€å§‹æ‰§è¡Œæ‰¹é‡æŠ¥è¡¨ä»»åŠ¡: {task_id}")
            
            # åˆ›å»ºä»»åŠ¡è¾“å‡ºç›®å½•
            task_output_dir = os.path.join(
                self.output_base_dir,
                f"task_{task_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
            os.makedirs(task_output_dir, exist_ok=True)
            
            # æ›´æ–°ä»»åŠ¡è¾“å‡ºç›®å½•
            crud_batch_reports.update_batch_report_task(
                self.db,
                task_id,
                crud_batch_reports.BatchReportTaskUpdate(output_directory=task_output_dir)
            )
            
            # è·å–ä»»åŠ¡é¡¹
            task_items = crud_batch_reports.get_batch_report_task_items(self.db, task_id)
            
            # æ‰§è¡Œæ¯ä¸ªæŠ¥è¡¨é¡¹
            generated_files = []
            for item in task_items:
                try:
                    file_path = await self._execute_report_item(task, item, task_output_dir)
                    if file_path:
                        generated_files.append(file_path)
                        
                        # æ›´æ–°ä»»åŠ¡é¡¹çŠ¶æ€ä¸ºå®Œæˆ
                        crud_batch_reports.update_batch_report_task_item(
                            self.db,
                            item.id,
                            BatchReportTaskItemUpdate(
                                status="completed",
                                completed_at=datetime.utcnow(),
                                file_path=file_path,
                                file_size=os.path.getsize(file_path) if os.path.exists(file_path) else 0,
                                file_format=task.export_config.get("export_format", "xlsx")
                            )
                        )
                    else:
                        # æ›´æ–°ä»»åŠ¡é¡¹çŠ¶æ€ä¸ºå¤±è´¥
                        crud_batch_reports.update_batch_report_task_item(
                            self.db,
                            item.id,
                            BatchReportTaskItemUpdate(
                                status="failed",
                                completed_at=datetime.utcnow(),
                                error_message="æŠ¥è¡¨ç”Ÿæˆå¤±è´¥"
                            )
                        )
                        
                except Exception as e:
                    logger.error(f"æ‰§è¡ŒæŠ¥è¡¨é¡¹å¤±è´¥ {item.id}: {str(e)}")
                    
                    # æ›´æ–°ä»»åŠ¡é¡¹çŠ¶æ€ä¸ºå¤±è´¥
                    crud_batch_reports.update_batch_report_task_item(
                        self.db,
                        item.id,
                        BatchReportTaskItemUpdate(
                            status="failed",
                            completed_at=datetime.utcnow(),
                            error_message=str(e)
                        )
                    )
            
            # å¦‚æœéœ€è¦æ‰“åŒ…ï¼Œåˆ›å»ºå‹ç¼©æ–‡ä»¶
            archive_file_path = None
            if task.export_config.get("include_archive", True) and generated_files:
                archive_file_path = await self._create_archive(task_id, task_output_dir, generated_files)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
            task_update = crud_batch_reports.BatchReportTaskUpdate(
                status="completed",
                completed_at=datetime.utcnow(),
                progress=100
            )
            
            if archive_file_path:
                task_update.archive_file_path = archive_file_path
                task_update.archive_file_size = os.path.getsize(archive_file_path)
            
            crud_batch_reports.update_batch_report_task(self.db, task_id, task_update)
            
            # åˆ›å»ºæ–‡ä»¶ç®¡ç†è®°å½•
            if archive_file_path:
                await self._create_file_record(task, archive_file_path, "archive")
            
            for file_path in generated_files:
                await self._create_file_record(task, file_path, "report")
            
            logger.info(f"æ‰¹é‡æŠ¥è¡¨ä»»åŠ¡æ‰§è¡Œå®Œæˆ: {task_id}, ç”Ÿæˆ {len(generated_files)} ä¸ªæ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"æ‰§è¡Œæ‰¹é‡æŠ¥è¡¨ä»»åŠ¡å¤±è´¥ {task_id}: {str(e)}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            crud_batch_reports.update_batch_report_task(
                self.db,
                task_id,
                crud_batch_reports.BatchReportTaskUpdate(
                    status="failed",
                    completed_at=datetime.utcnow(),
                    error_message=str(e)
                )
            )
    
    async def _execute_report_item(
        self,
        task: BatchReportTask,
        item: BatchReportTaskItem,
        output_dir: str
    ) -> Optional[str]:
        """
        æ‰§è¡Œå•ä¸ªæŠ¥è¡¨é¡¹
        
        Args:
            task: æ‰¹é‡æŠ¥è¡¨ä»»åŠ¡
            item: æŠ¥è¡¨ä»»åŠ¡é¡¹
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„æˆ–None
        """
        try:
            # æ›´æ–°ä»»åŠ¡é¡¹çŠ¶æ€ä¸ºè¿è¡Œä¸­
            crud_batch_reports.update_batch_report_task_item(
                self.db,
                item.id,
                BatchReportTaskItemUpdate(
                    status="running",
                    started_at=datetime.utcnow()
                )
            )
            
            logger.info(f"å¼€å§‹æ‰§è¡ŒæŠ¥è¡¨é¡¹: {item.id} - {item.report_name}")
            
            # æ ¹æ®æŠ¥è¡¨ç±»å‹ç”ŸæˆæŠ¥è¡¨
            file_path = None
            report_config = item.report_config
            export_format = task.export_config.get("export_format", "xlsx")
            
            # ğŸš€ å¢å¼ºçš„æŠ¥è¡¨ç±»å‹æ˜ å°„é€»è¾‘
            if item.report_type in self.report_type_mapping:
                report_type = self.report_type_mapping[item.report_type]
                file_path = await self._generate_report(report_type, report_config, output_dir, export_format)
            else:
                # ğŸ’¡ æ™ºèƒ½å›é€€ç­–ç•¥ï¼šæ ¹æ®æŠ¥è¡¨åç§°æ¨æ–­æŠ¥è¡¨ç±»å‹
                logger.warning(f"æœªçŸ¥çš„æŠ¥è¡¨ç±»å‹: {item.report_type}ï¼Œå°è¯•æ™ºèƒ½æ¨æ–­...")
                
                report_type = self._infer_report_type_from_name(item.report_name, item.report_type)
                if report_type:
                    logger.info(f"æ™ºèƒ½æ¨æ–­æŠ¥è¡¨ç±»å‹: {item.report_type} -> {report_type}")
                    file_path = await self._generate_report(report_type, report_config, output_dir, export_format)
                else:
                    # æœ€åçš„å›é€€ï¼šä½¿ç”¨é»˜è®¤çš„è–ªèµ„æ˜ç»†ç”Ÿæˆå™¨
                    logger.warning(f"æ— æ³•æ¨æ–­æŠ¥è¡¨ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤ç”Ÿæˆå™¨: {item.report_type}")
                    file_path = await self._generate_report("payroll_detail", report_config, output_dir, export_format)
            
            logger.info(f"æŠ¥è¡¨é¡¹æ‰§è¡Œå®Œæˆ: {item.id} - {file_path}")
            return file_path
            
        except Exception as e:
            logger.error(f"æ‰§è¡ŒæŠ¥è¡¨é¡¹å¤±è´¥ {item.id}: {str(e)}")
            raise
    
    def _infer_report_type_from_name(self, report_name: str, report_type: str) -> Optional[str]:
        """
        ğŸ” æ ¹æ®æŠ¥è¡¨åç§°å’Œç±»å‹æ™ºèƒ½æ¨æ–­å¯¹åº”çš„ç”Ÿæˆå™¨ç±»å‹
        
        Args:
            report_name: æŠ¥è¡¨åç§°
            report_type: åŸå§‹æŠ¥è¡¨ç±»å‹ç¼–ç 
            
        Returns:
            æ¨æ–­çš„ç”Ÿæˆå™¨ç±»å‹ï¼Œå¦‚æœæ— æ³•æ¨æ–­åˆ™è¿”å›None
        """
        name_lower = report_name.lower()
        type_lower = report_type.lower()
        
        # å…³é”®è¯æ˜ å°„è§„åˆ™
        if any(keyword in name_lower for keyword in ['å®å‘', 'æ˜ç»†', 'shi_fa', 'detail']):
            return "payroll_detail"
        elif any(keyword in name_lower for keyword in ['æ±‡æ€»', 'å¯¼å…¥', 'å¤§å¹³å°', 'hui_zong', 'summary']):
            return "department_summary"  
        elif any(keyword in type_lower for keyword in ['pin_yong', 'è˜ç”¨', 'payroll']):
            return "payroll_summary"
        elif any(keyword in name_lower for keyword in ['ç¤¾ä¿', 'å…¬ç§¯é‡‘', 'å¹´é‡‘', 'social', 'insurance']):
            return "social_insurance"
        elif any(keyword in name_lower for keyword in ['ä¸ªç¨', 'ç¨', 'tax']):
            return "tax_report"
        elif any(keyword in name_lower for keyword in ['è€ƒå‹¤', 'attendance']):
            return "attendance_summary"
        
        # æ— æ³•æ¨æ–­ï¼Œè¿”å›None
        return None
    
    async def _generate_report(
        self,
        report_type: str,
        config: Dict[str, Any],
        output_dir: str,
        export_format: str
    ) -> str:
        """ç”ŸæˆæŠ¥è¡¨"""
        try:
            generator = None
            if report_type == "payroll_summary":
                generator = PayrollSummaryGenerator(self.db)
            elif report_type == "payroll_detail":
                generator = PayrollDetailGenerator(self.db)
            elif report_type == "department_summary":
                generator = DepartmentSummaryGenerator(self.db)
            elif report_type == "tax_report":
                generator = TaxDeclarationGenerator(self.db)
            elif report_type == "social_insurance":
                generator = SocialInsuranceGenerator(self.db)
            elif report_type == "attendance_summary":
                generator = AttendanceSummaryGenerator(self.db)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æŠ¥è¡¨ç±»å‹: {report_type}")
            
            return generator.generate_report(config, output_dir, export_format)
        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥è¡¨å¤±è´¥: {str(e)}")
            raise
    
    async def _create_archive(
        self,
        task_id: int,
        output_dir: str,
        file_paths: List[str]
    ) -> str:
        """
        åˆ›å»ºå‹ç¼©æ–‡ä»¶
        
        Args:
            task_id: ä»»åŠ¡ID
            output_dir: è¾“å‡ºç›®å½•
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            å‹ç¼©æ–‡ä»¶è·¯å¾„
        """
        try:
            archive_name = f"æ‰¹é‡æŠ¥è¡¨_{task_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
            archive_path = os.path.join(output_dir, archive_name)
            
            with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for file_path in file_paths:
                    if os.path.exists(file_path):
                        # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºå‹ç¼©åŒ…å†…çš„è·¯å¾„
                        arcname = os.path.basename(file_path)
                        zipf.write(file_path, arcname)
            
            logger.info(f"åˆ›å»ºå‹ç¼©æ–‡ä»¶æˆåŠŸ: {archive_path}")
            return archive_path
            
        except Exception as e:
            logger.error(f"åˆ›å»ºå‹ç¼©æ–‡ä»¶å¤±è´¥: {str(e)}")
            raise
    
    async def _create_file_record(
        self,
        task: BatchReportTask,
        file_path: str,
        file_type: str
    ) -> None:
        """
        åˆ›å»ºæ–‡ä»¶ç®¡ç†è®°å½•
        
        Args:
            task: æ‰¹é‡æŠ¥è¡¨ä»»åŠ¡
            file_path: æ–‡ä»¶è·¯å¾„
            file_type: æ–‡ä»¶ç±»å‹
        """
        try:
            file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
            file_format = os.path.splitext(file_path)[1][1:] if '.' in file_path else None
            
            # è®¡ç®—è¿‡æœŸæ—¶é—´
            auto_cleanup_hours = task.export_config.get("auto_cleanup_hours", 24)
            expires_at = datetime.utcnow() + timedelta(hours=auto_cleanup_hours)
            
            file_data = ReportFileManagerCreate(
                file_name=os.path.basename(file_path),
                file_path=file_path,
                file_size=file_size,
                file_type=file_type,
                file_format=file_format,
                source_type="batch_task",
                source_id=task.id,
                access_level="private",
                is_temporary=True,
                expires_at=expires_at,
                auto_cleanup=True,
                metadata_info={
                    "task_id": task.id,
                    "task_name": task.task_name,
                    "generated_at": datetime.utcnow().isoformat()
                }
            )
            
            crud_batch_reports.create_report_file(
                self.db,
                file_data,
                task.created_by
            )
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ–‡ä»¶è®°å½•å¤±è´¥: {str(e)}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹ 